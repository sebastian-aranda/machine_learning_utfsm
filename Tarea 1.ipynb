{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Regresión Lineal Ordinaria (LSS)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Se elimina id, date y zipcode ya que no aportan informacion relevante en cuanto al precio de las casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df.drop(['id','date','zipcode',],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) El dataset consta de 18 columnas o atributos, las cuales presentan valores no nulos y enteros o reales, un total de 21.613 instancias de casas.\n",
    "Todas las casas se encuentran localizadas en King County, esto se refleja en la media de la latitud y longitud (47.560053, -122.213896) y su baja desviasión estándar, la mayoria presenta entre 3 a 4 dormitorios y 1 a 2 baños, los demás datos presentan mayor dispersión por lo que no son muy útiles para generalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Esta parte corresponde al preprocesamiento de los datos, y es utilizado cuando los datos tienen valores muy dispersos (en este caso casi todos exceptuando: 'bedrooms', 'bathrooms', 'lat', 'long'). La motivación detrás de esto radica en que algunos algoritmos, la función objetivo no funciona muy bien sin antes realizar normalización, es decir los datos deben comportarse parecido a una distribución normal estándar.\n",
    "Luego se aplica función logaritmo al valor del precio ya que lo que nos interesa son los cambios relativos de este valor según los atributos en juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df_scaled['price'] = np.log(df['price'])\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Los datos ya se han centrado (paso anterior) por lo que el parametro 'fit_intercept' no se utilizará.\n",
    "Luego nos interesan todas las columnas exceptuando 'price' ya que solo se quiere incluir los parámetros como los valores de $X$. La linea 4 añade la columna nº18 (luego de eliminar la columna *'price'*) de intercepto, este paso es el mismo realizado en clases en donde el intercepto de la ecuación lineal que representa el comportamiento del precio de acuerdo a los atributos es añadido al dataset o matriz de parámetros $X$ con el fin de encapsular el parámetro desconocido (*$b_0$*) en un solo vector (columna).\n",
    "\n",
    "Como se mencionó anteriormente, *'fit_intercept'* no se utilizará ya que el intercepto es eliminado de la ecuación lineal e integrado a la matriz de parámetros por lo tanto en los argumentos de la función que implementa la regresión lineal no será necesario. Luego *'normalize'* es ignorado ya que esto se realizo en el paso anterior y *'n_jobs'* es utilizado para dividir el trabajo en multiples CPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "X = df_scaled.iloc[:,1:] #use .ix instead, in older pandas version //Se consideran todas las filas y columnas menos 'price'\n",
    "N = X.shape[0] #Cantidad de filas\n",
    "X.insert(X.shape[1], 'intercept', np.ones(N)) #X.shape[1] = dimensionalidad de datos (18, incluyendo intercepto)\n",
    "y = df_scaled['price']\n",
    "\n",
    "#mascara estatica con el 70% de los datos\n",
    "mascara = np.zeros(len(X))\n",
    "limit = int(len(X)*0.7)\n",
    "mascara[:limit] = 1\n",
    "istrain = mascara == 1\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "\n",
    "linreg = lm.LinearRegression(fit_intercept = False)\n",
    "linreg.fit(Xtrain, ytrain)\n",
    "#predictions_train = linreg.predict(x_train)\n",
    "#residuals_train = predictions_train - ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linreg.score(Xtest, ytest) #Coeficiente de determinacion R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Las variables con mayor correlación con la respuesta son *'sqft_above'*, *'sqft_basement'* y *'sqft_living'* ya que presentan el mayor peso absoluto. Luego, un atributo con $z-score$ mayor a 1.960 en valor absoluto es importante a un nivel de significancia del 5%, por lo tanto *'sqft_living'* y *'sqft_above'* serían las variables con mayor influencia, esto es causado ya que son los atributos que mas impacto tienen en el precio final de las casas, al calcular el $z-score$ con 5% de significancia tenemos que con un 95% de confiabilidad los atributos son relevantes, y esto se sostiene hasta con un nivel de significancia del 1% (|z-score| > 2.576)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from scipy.stats import t\n",
    "df_e = pd.DataFrame(zip(X.columns, linreg.coef_,), columns = ['Atributos', 'Pesos'])\n",
    "zscore = df_e[df_e.select_dtypes(include=[np.number]).columns].apply(zscore)\n",
    "df_e.insert(df_e.iloc[:,:].shape[1], \"Z-scores\", zscore)\n",
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_test = linreg.predict(Xtest)\n",
    "mse_test = np.mean(np.power(yhat_test - ytest, 2))\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "linreg = lm.LinearRegression(fit_intercept = False)\n",
    "linreg.fit(Xm[train], ym[train])\n",
    "yhat_val = linreg.predict(Xm[val])\n",
    "mse_fold = np.mean(np.power(yhat_val - ym[val], 2))\n",
    "mse_cv += mse_fold\n",
    "mse_cv = mse_cv / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Selección de Atributos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fss(x, y, names_x, k = 10000):\n",
    "    p = x.shape[1]-1\n",
    "    k = min(p, k)\n",
    "    names_x = np.array(names_x)\n",
    "    remaining = range(0, p)\n",
    "    selected = [p]\n",
    "    current_score = best_new_score = 0.0\n",
    "    while remaining and len(selected)<=k :\n",
    "        score_candidates = []\n",
    "        for candidate in remaining:\n",
    "            model = lm.LinearRegression(fit_intercept=False)\n",
    "            indexes = selected + [candidate]\n",
    "            x_train = x[:,indexes]\n",
    "            predictions_train = model.fit(x_train, y).predict(x_train)\n",
    "            residuals_train = predictions_train - y\n",
    "            mse_candidate = np.mean(np.power(residuals_train, 2))\n",
    "            score_candidates.append((mse_candidate, candidate))\n",
    "        score_candidates.sort()\n",
    "        score_candidates[:] = score_candidates[::-1]\n",
    "        best_new_score, best_candidate = score_candidates.pop()\n",
    "        remaining.remove(best_candidate)\n",
    "        selected.append(best_candidate)\n",
    "        print \"selected = %s ...\"%names_x[best_candidate]\n",
    "        print \"totalvars=%d, mse = %f\"%(len(indexes),best_new_score)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected = grade ...\n",
      "totalvars=2, mse = 0.143260\n",
      "selected = lat ...\n",
      "totalvars=3, mse = 0.103340\n",
      "selected = sqft_living ...\n",
      "totalvars=4, mse = 0.084960\n",
      "selected = yr_built ...\n",
      "totalvars=5, mse = 0.075447\n",
      "selected = view ...\n",
      "totalvars=6, mse = 0.071782\n",
      "selected = sqft_living15 ...\n",
      "totalvars=7, mse = 0.069979\n",
      "selected = bathrooms ...\n",
      "totalvars=8, mse = 0.068251\n",
      "selected = condition ...\n",
      "totalvars=9, mse = 0.066864\n",
      "selected = waterfront ...\n",
      "totalvars=10, mse = 0.065836\n",
      "selected = floors ...\n",
      "totalvars=11, mse = 0.065107\n",
      "selected = sqft_lot ...\n",
      "totalvars=12, mse = 0.064820\n",
      "selected = yr_renovated ...\n",
      "totalvars=13, mse = 0.064619\n",
      "selected = bedrooms ...\n",
      "totalvars=14, mse = 0.064583\n",
      "selected = sqft_above ...\n",
      "totalvars=15, mse = 0.064554\n",
      "selected = sqft_lot15 ...\n",
      "totalvars=16, mse = 0.064527\n",
      "selected = long ...\n",
      "totalvars=17, mse = 0.064516\n",
      "selected = sqft_basement ...\n",
      "totalvars=18, mse = 0.064516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17, 8, 13, 2, 11, 6, 15, 1, 7, 5, 4, 3, 12, 0, 9, 16, 14, 10]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df.drop(['id','date','zipcode',],axis=1,inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df_scaled['price'] = np.log(df['price'])\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "X = df_scaled.iloc[:,1:]\n",
    "N = X.shape[0] #Cantidad de filas\n",
    "X.insert(X.shape[1], 'intercept', np.ones(N))\n",
    "y = df_scaled['price']\n",
    "\n",
    "mascara = np.zeros(len(X))\n",
    "limit = int(len(X)*0.7)\n",
    "mascara[:limit] = 1\n",
    "istrain = mascara == 1\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "\n",
    "Xm = Xtrain.as_matrix()\n",
    "ym = ytrain.as_matrix()\n",
    "\n",
    "names_regressors = X.columns[:-1]\n",
    "fss(Xm,ym,names_regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Regularización</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Drift</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Detectar enfermedades cardiacas</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se debe realizar un preprocesamiento del dataset para que el modelo de Regresión Lineal se ajuste correctamente a los datos; en particular se deben crear nuevas columnas para todas las variables categóricas. Al realizar esto se asegura de que el modelo no considere estos valores como ordenados, es decir, si una variable posee 3 categorías codificads como {1,2,3}, hay que \"explicarle\" al modelo que tener un valor 1 no significa que sea menor que tener un valor 3, son solo valores categóricos distintos independientes entre ellos.\n",
    "\n",
    "Por ejemplo, para la variable 'chest_pain' se crean 3 nuevas columnas en donde se codifica la presencia (con 0 y 1) de cada uno de los posibles valores en su dominio, por lo tanto queda claro que la variable 'chest_pain' posee 3 valores distintos.\n",
    "\n",
    "Además de esto, y al igual que en la sección 1, se debe realizar una estandarización de los datos para asegurarnos de que nuestros valores reales se muevan en un espacio similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "headers = ['age','sex','chest_pain','blood_p','serum','blood_s','electro','max_heart', 'angina','oldpeak','slope',\\\n",
    "           'vessel','thal','normal']\n",
    "\n",
    "df = pd.read_csv(\"heart.dat\", header=None, names=headers, sep=' ')\n",
    "#df.head()\n",
    "\n",
    "#Data standardization\n",
    "df_scaled = pd.concat([df['age'],df['blood_p'],df['serum'],df['max_heart'],df['oldpeak'],df['slope'],df['vessel']],axis=1)\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_scaled), columns=df_scaled.columns)\n",
    "\n",
    "#Dataframes with binary coded categorical variables\n",
    "df_sex_dummies = pd.get_dummies(df['sex'], prefix='sex')\n",
    "df_chest_pain_dummies = pd.get_dummies(df['chest_pain'], prefix='chest_pain')\n",
    "df_electro_dummies = pd.get_dummies(df['electro'], prefix='electro')\n",
    "df_thal_dummies = pd.get_dummies(df['thal'], prefix='thal')\n",
    "df_categorical = pd.concat([df_sex_dummies,df_chest_pain_dummies, df_electro_dummies, df['blood_s'], df['angina'],df_thal_dummies], axis=1)\n",
    "\n",
    "#Dataframe for coding categorical output\n",
    "df_predict_dummies = pd.get_dummies(df['normal'], prefix='normal')\n",
    "df_predict_dummies.drop(['normal_1'],axis=1,inplace=True)\n",
    "df_predict_dummies.columns.values[0] = 'normal'\n",
    "\n",
    "#Merging both dataframes with al preprocessed data\n",
    "df = pd.concat([df_scaled,df_categorical, df_predict_dummies], axis=1)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.913580246914\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "y = df['normal']\n",
    "df.drop(['normal'],axis=1,inplace=True)\n",
    "\n",
    "X = df.iloc[:,:]\n",
    "N = X.shape[0] #Cantidad de filas\n",
    "X.insert(X.shape[1], 'intercept', np.ones(N))\n",
    "\n",
    "#mascara estatica con el 70% de los datos\n",
    "mascara = np.zeros(len(X))\n",
    "limit = int(len(X)*0.7)\n",
    "mascara[:limit] = 1\n",
    "istrain = mascara == 1\n",
    "Xtrain = X[istrain]\n",
    "ytrain = y[istrain]\n",
    "Xtest = X[np.logical_not(istrain)]\n",
    "ytest = y[np.logical_not(istrain)]\n",
    "\n",
    "linreg = lm.LinearRegression(fit_intercept = False)\n",
    "linreg.fit(Xtrain, ytrain)\n",
    "\n",
    "y_hat = linreg.predict(Xtest)\n",
    "y_hat_categorical = []\n",
    "for val in y_hat:\n",
    "    if val > 0.5:\n",
    "        y_hat_categorical.append(1)\n",
    "    else:\n",
    "        y_hat_categorical.append(0)\n",
    "\n",
    "#mse_matriz = np.power(y_outlier - y_predict_outlier, 2)\n",
    "#mse_matriz = pd.DataFrame(mse_matriz)\n",
    "#mse_matriz.describe()\n",
    "\n",
    "ytest = ytest.tolist()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Score: \"+str(accuracy_score(ytest,y_hat_categorical)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
